<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    
    <!-- Website verification -->
    <meta name="google-site-verification" content="-BO_S6ChfLrtygIHRRWdOiMoyOiDO91B3hNAFN88sFA">
<!-- Avoid warning on Google Chrome
        Error with Permissions-Policy header: Origin trial controlled feature not enabled: 'interest-cohort'.
        see https://stackoverflow.com/a/75119417
    -->
    <meta http-equiv="Permissions-Policy" content="interest-cohort=()">

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Diana F. Sousa, PhD</title>
    <meta name="author" content="Diana F. Sousa, PhD">
    <meta name="description" content="Aaditya Ura | ML Researcher
">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon.ico">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://dpavot.github.io/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <!-- Social Icons -->
          <div class="navbar-brand social">
            <a href="mailto:%44%69%61%6E%61.%66%72%61%6E%63%69%73%63%6F-%64%65-%73%6F%75%73%61@%65%63.%65%75%72%6F%70%61.%65%75" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://orcid.org/0000-0003-0597-9273" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a>
            <a href="https://scholar.google.com/citations?user=6CTPQnsAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://www.researchgate.net/profile/Diana-Sousa-6/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a>
            <a href="https://github.com/dpavot" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/diana-sousa" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a>
            <a href="https://twitter.com/dianafdsousa" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a>
            <a href="https://dblp.org/pid/351/3594.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a>
            

          </div>
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">codes</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/talks/">talks</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
            <span style="font-weight: 500">Diana</span>
              F.
              Sousa, PhDÂ Â <br class="d-md-none">
          </h1>
          <p class="desc"></p>
        </header>

        <article>
          <div class="profile float-right">

              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

            <div class="address d-none d-md-block">
              <p>aadityaura@gmail.com</p>

            </div>
          </div>

          <div class="clearfix">
            <p>Hi there, Iâ€™m Aaditya (Ankit), a Senior Research Engineer specializing in NLP, Deep learning, and Machine learning. My research aims to develop machine learning methods in the Healthcare domain.</p>

<p>I currently work at <a href="https://www.saama.com/" rel="external nofollow noopener" target="_blank">Saama</a>, conducting research to accelerate <a href="https://www.pfizer.com/news/articles/how_a_novel_incubation_sandbox_helped_speed_up_data_analysis_in_pfizer_s_covid_19_vaccine_trial" rel="external nofollow noopener" target="_blank">clinical trials and reduce drug development timelines</a>. Before joining Saama, I worked as an NLP and Backend Engineer at <a href="https://prescienceds.com/" rel="external nofollow noopener" target="_blank">Prescience</a>. My research interests involve Representation Learning on Graphs, Federated learning, XAI, Generative language models and their applications in Healthcare data.</p>

<p>Iâ€™ve been honored to contribute meaningful research and datasets that have been adopted by leading companies like <a href="https://arxiv.org/abs/2211.09085" rel="external nofollow noopener" target="_blank">Facebook AI</a> (Galactica), <a href="https://arxiv.org/abs/2305.09617" rel="external nofollow noopener" target="_blank">Google AI</a> (Med-PaLM, Med-PaLM-2), <a href="https://arxiv.org/abs/2303.13375" rel="external nofollow noopener" target="_blank">Microsoft, and OpenAI</a> (GPT-4) to further responsible advancements in AI.</p>

<p>I have a deep appreciation for open-source work and contribute to projects including Tensorflow, Pytorch Geometric and HuggingFace. In December 2022, I noticed structured output issues in large language models and developed <a href="https://github.com/promptslab/Promptify" rel="external nofollow noopener" target="_blank">Promptify</a>, which received encouraging feedback on GitHub and assisted relief efforts during the <a href="https://dev.to/erayg/how-an-open-source-disaster-map-helped-thousands-of-earthquake-survivors-afetharitacom-440" rel="external nofollow noopener" target="_blank">Turkey-Syria earthquake</a>. During COVID-19 pandemic in March 2020, I initiated a project to capture cough and breath sounds via phone to <a href="https://arxiv.org/abs/2010.02417" rel="external nofollow noopener" target="_blank">classify COVID-19 coughs using deep learning</a>, with the aim of aiding doctors in rapid pre-screening of patients.</p>

<p>If you are still reading, here is more about me: Apart from my research life, Iâ€™m drawn to activities like Boxing, Jiu-Jitsu and Chess. I enjoy spending time in nature, Observation, and Philosophy. I have realized to some extent that everything is connected; there is neither good nor bad; there is neither positive nor negative. I often displace myself away from social noise, take a seat, and try to see, observe, rather than just looking at it. In the process, I naturally learn.</p>

<!-- I was also privileged to serve as a reviewer for journals [Springer Nature 2021](https://www.webofscience.com/wos/author/record/AAA-9381-2021), [IEEE Access 2021](https://www.webofscience.com/wos/author/record/AAA-9381-2021) and [IEEE Access 2022](https://www.webofscience.com/wos/author/record/AAA-9381-2021). -->

<!--
Write your biography here. Tell the world about yourself. Link to your favorite [subreddit](http://reddit.com). You can put a picture in, too. The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder.test

Put your address / P.O. box / other info right below your picture. You can also disable any of these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

Link to your social media connections, too. This theme is set up to use [Font Awesome icons](http://fortawesome.github.io/Font-Awesome/) and [Academicons](https://jpswalsh.github.io/academicons/), like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them.
-->

          </div>

          <!-- News -->
          <h2><a href="/news/" style="color: inherit;">news</a></h2>          <div class="news">
            <div class="table-responsive" style="max-height: 60vw">
              <table class="table table-sm table-borderless">
              
                <tr>
                  <th scope="row">Oct 10, 2023</th>
                  <td>
                    Our work on <a href="https://arxiv.org/abs/2307.15343" rel="external nofollow noopener" target="_blank">Hallucination Test for Large Language Models</a> was accepted at EMNLP(Conll) 2023.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Jul 29, 2023</th>
                  <td>
                    Weâ€™re thrilled to announce the release of <a href="https://github.com/promptslab/Promptify/" rel="external nofollow noopener" target="_blank">Promptify 2.0</a> ðŸŽ‰ - to deal with the structured output issue in LLMs. 
Promptify is trending on GitHub! âœ¨


                  </td>
                </tr>
                <tr>
                  <th scope="row">Nov 23, 2022</th>
                  <td>
                    Our work on Distribution Shift on Question Answering Models was accepted at NeurIPS (Robustness in Sequence Modeling) 2022.


                  </td>
                </tr>
                <tr>
                  <th scope="row">Oct 1, 2022</th>
                  <td>
                    Our work on <a href="https://dl.acm.org/doi/10.1145/3533708" rel="external nofollow noopener" target="_blank">Federated Learning in Healthcare domain</a> was accepted at ACM 2022.


                  </td>
                </tr>
                <tr>
                  <th scope="row">Apr 7, 2022</th>
                  <td>
                    Our work on Open-domain Question Answering in Medical domain was accepted at Conference on Health, Inference, and Learning (CHIL) 2022.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Dec 11, 2020</th>
                  <td>
                    We worked on ML based discrepancy identification and data reconciliation tool Smart Data Query &amp; Smart Auto Mapper to accelerate COVID vaccine trial. Full details can be found at <a href="https://www.pfizer.com/news/articles/how\_a\_novel\_incubation\_sandbox\_helped\_speed\_up\_data\_analysis\_in\_pfizer\_s\_covid\_19\_vaccine\_trial" rel="external nofollow noopener" target="_blank">Pfizer page</a>

                  </td>
                </tr>
              </table>
            </div>
          </div>

          <!-- Latest posts -->
          

          <!-- Selected papers -->
          <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2>
          <div class="publications">
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/"><div id="-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('-modal').style.display='none'">Ã—</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('');
      var modalImg = document.getElementById("-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="SemEval" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">ULISBOA at SemEval-2017 Task 12: Extraction and classification of temporal expressions and events</div>
    <!-- Author -->
    <div class="author">
      

      </div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</em>
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://aclanthology.org/S17-2179/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://aclanthology.org/S17-2179.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="https://github.com/lasigeBioTM/CiTA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">Slides</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>This paper presents our approach to participate in the SemEval 2017 Task 12: Clinical TempEval challenge, specifically in the event and time expressions span and attribute identification subtasks (ES, EA, TS, TA). Our approach consisted in training Conditional Random Fields (CRF) classifiers using the provided annotations, and in creating manually curated rules to classify the attributes of each event and time expression. We used a set of common features for the event and time CRF classifiers, and a set of features specific to each type of entity, based on domain knowledge. Training only on the source domain data, our best F-scores were 0.683 and 0.485 for event and time span identification subtasks. When adding target domain annotations to the training data, the best F-scores obtained were 0.729 and 0.554, for the same subtasks. We obtained the second highest F-score of the challenge on the event polarity subtask (0.708). The source code of our system, Clinical Timeline Annotation (CiTA), is available at https://github.com/lasigeBioTM/CiTA.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/"><div id="-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('-modal').style.display='none'">Ã—</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('');
      var modalImg = document.getElementById("-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="BO-LSTM" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">BO-LSTM: classifying relations via long short-term memory networks along biomedical ontologies</div>
    <!-- Author -->
    <div class="author">
      

      Luka A. Clarke Andre Lamurias,Â andÂ Francisco M. Couto</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>BMC Bioinformatics</em>, 2019
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2584-5" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://bmcbioinformatics.biomedcentral.com/counter/pdf/10.1186/s12859-018-2584-5.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">Slides</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Recent studies have proposed deep learning techniques, namely recurrent neural networks, to improve biomedical text mining tasks. However, these techniques rarely take advantage of existing domain-specific resources, such as ontologies. In Life and Health Sciences there is a vast and valuable set of such resources publicly available, which are continuously being updated. Biomedical ontologies are nowadays a mainstream approach to formalize existing knowledge about entities, such as genes, chemicals, phenotypes, and disorders. These resources contain supplementary information that may not be yet encoded in training data, particularly in domains with limited labeled data. We propose a new model to detect and classify relations in text, BO-LSTM, that takes advantage of domain-specific ontologies, by representing each entity as the sequence of its ancestors in the ontology. We implemented BO-LSTM as a recurrent neural network with long short-term memory units and using open biomedical ontologies, specifically Chemical Entities of Biological Interest (ChEBI), Human Phenotype, and Gene Ontology. We assessed the performance of BO-LSTM with drug-drug interactions mentioned in a publicly available corpus from an international challenge, composed of 792 drug descriptions and 233 scientific abstracts. By using the domain-specific ontology in addition to word embeddings and WordNet, BO-LSTM improved the F1-score of both the detection and classification of drug-drug interactions, particularly in a document set with a limited number of annotations. We adapted an existing DDI extraction model with our ontology-based method, obtaining a higher F1 score than the original model. Furthermore, we developed and made available a corpus of 228 abstracts annotated with relations between genes and phenotypes, and demonstrated how BO-LSTM can be applied to other types of relations. Our findings demonstrate that besides the high performance of current deep learning techniques, domain-specific ontologies can still be useful to mitigate the lack of labeled data.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/"><div id="-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('-modal').style.display='none'">Ã—</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('');
      var modalImg = document.getElementById("-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="PGR" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">A Silver Standard Corpus of Human Phenotype-Gene Relations</div>
    <!-- Author -->
    <div class="author">
      

      Francisco M. Couto Diana F. Sousa</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://aclanthology.org/N19-1152/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://aclanthology.org/N19-1152.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">Slides</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Human phenotype-gene relations are fundamental to fully understand the origin of some phenotypic abnormalities and their associated diseases. Biomedical literature is the most comprehensive source of these relations, however, we need Relation Extraction tools to automatically recognize them. Most of these tools require an annotated corpus and to the best of our knowledge, there is no corpus available annotated with human phenotype-gene relations. This paper presents the Phenotype-Gene Relations (PGR) corpus, a silver standard corpus of human phenotype and gene annotations and their relations. The corpus consists of 1712 abstracts, 5676 human phenotype annotations, 13835 gene annotations, and 4283 relations. We generated this corpus using Named-Entity Recognition tools, whose results were partially evaluated by eight curators, obtaining a precision of 87.01%. By using the corpus we were able to obtain promising results with two state-of-the-art deep learning tools, namely 78.05% of precision. The PGR corpus was made publicly available to the research community.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/"><div id="-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('-modal').style.display='none'">Ã—</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('');
      var modalImg = document.getElementById("-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="BiOnt" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">BiOnt: Deep Learning Using Multiple Biomedical Ontologies for Relation Extraction</div>
    <!-- Author -->
    <div class="author">
      

      Diana F. Sousa,Â andÂ Francisco M. Couto</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Advances in Information Retrieval: 42nd European Conference on IR Research</em>, 2020
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://link.springer.com/chapter/10.1007/978-3-030-45442-5_46" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://link.springer.com/content/pdf/10.1007/978-3-030-45442-5_46.pdf?pdf=inline%20link" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="https://github.com/lasigeBioTM/BiOnt" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">Slides</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Successful biomedical relation extraction can provide evidence to researchers and clinicians about possible unknown associations between biomedical entities, advancing the current knowledge we have about those entities and their inherent mechanisms. Most biomedical relation extraction systems do not resort to external sources of knowledge, such as domain-specific ontologies. However, using deep learning methods, along with biomedical ontologies, has been recently shown to effectively advance the biomedical relation extraction field. To perform relation extraction, our deep learning system, BiOnt, employs four types of biomedical ontologies, namely, the Gene Ontology, the Human Phenotype Ontology, the Human Disease Ontology, and the Chemical Entities of Biological Interest, regarding gene-products, phenotypes, diseases, and chemical compounds, respectively. We tested our system with three data sets that represent three different types of relations of biomedical entities. BiOnt achieved, in F-score, an improvement of 4.93 percentage points for drug-drug interactions (DDI corpus), 4.99 percentage points for phenotype-gene relations (PGR corpus), and 2.21 percentage points for chemical-induced disease relations (BC5CDR corpus), relatively to the state-of-the-art. The code supporting this system is available at https://github.com/lasigeBioTM/BiOnt.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/"><div id="-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('-modal').style.display='none'">Ã—</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('');
      var modalImg = document.getElementById("-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="Geonm" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Improving accessibility and distinction between negative results in biomedical relation extraction</div>
    <!-- Author -->
    <div class="author">
      

      Francisco M. Couto Diana F. Sousa</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Genomics &amp; Informatics</em>, 2020
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://genominfo.org/journal/view.php?doi=10.5808/GI.2020.18.2.e20" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://genominfo.org/upload/pdf/gi-2020-18-2-e20.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">Slides</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Accessible negative results are relevant for researchers and clinicians not only to limit their search space but also to prevent the costly re-exploration of research hypotheses. However, most biomedical relation extraction datasets do not seek to distinguish between a false and a negative relation among two biomedical entities. Furthermore, datasets created using distant supervision techniques also have some false negative relations that constitute undocumented/unknown relations (missing from a knowledge base). We propose to improve the distinction between these concepts, by revising a subset of the relations marked as false on the phenotype-gene relations corpus and give the first steps to automatically distinguish between the false (F), negative (N), and unknown (U) results. Our work resulted in a sample of 127 manually annotated FNU relations and a weighted-F1 of 0.5609 for their automatic distinction. This work was developed during the 6th Biomedical Linked Annotation Hackathon (BLAH6).</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/"><div id="-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('-modal').style.display='none'">Ã—</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('');
      var modalImg = document.getElementById("-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="neural_networks_book" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Using Neural Networks for Relation Extraction from Biomedical Literature</div>
    <!-- Author -->
    <div class="author">
      

      Francisco M. Couto Diana F. Sousa</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Artificial Neural Networks. Methods in Molecular Biology</em>, 2020
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://link.springer.com/protocol/10.1007/978-1-0716-0826-5_14" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p></p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 abbr"><abbr class="badge">IEEE</abbr></div>

  <!-- Entry bib key -->
  <div id="ieee-qa" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Generating Biomedical Question Answering Corpora From Q&amp;A Forums</div>
    <!-- Author -->
    <div class="author">
      

      Francisco M. Couto Andre Lamurias</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>IEEE Access</em>, 2020
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://ieeexplore.ieee.org/document/9184044" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9184044" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="https://github.com/lasigeBioTM/BiQA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">Slides</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Question Answering (QA) is a natural language processing task that aims at obtaining relevant answers to user questions. While some progress has been made in this area, biomedical questions are still a challenge to most QA approaches, due to the complexity of the domain and limited availability of training sets. We present a method to automatically extract question-article pairs from Q&amp;A web forums, which can be used for document retrieval, a crucial step of most QA systems. The proposed framework extracts from selected forums the questions and the respective answers that contain citations. This way, QA systems based on document retrieval can be developed and evaluated using the question-article pairs annotated by users of these forums. We generated the BiQA corpus by applying our framework to three forums, obtaining 7,453 questions and 14,239 question-article pairs. We evaluated how the number of articles associated with each question and the number of votes on each answer affects the performance of baseline document retrieval approaches. Also, we demonstrated that the articles given as answers are significantly similar to the questions and trained a state-of-the-art deep learning model that obtained similar performance to using a dataset manually annotated by experts. The proposed framework can be used to update the BiQA corpus from the same forums as new posts are made, and from other forums that support their answers with documents. The BiQA corpus and the framework used to generate it are available at https://github.com/lasigeBioTM/BiQA.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div>

  <!-- Entry bib key -->
  <div id="emnlp" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">COVID-19: A Semantic-Based Pipeline for Recommending Biomedical Entities</div>
    <!-- Author -->
    <div class="author">
      

      </div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Proceedings of the 1st Workshop on NLP for COVID-19 (Part 2)</em>, 2020
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://aclanthology.org/2020.nlpcovid19-2.20/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://aclanthology.org/2020.nlpcovid19-2.20.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>With the increasing number of publications about COVID-19, it is a challenge to extract personalized knowledge suitable for each researcher. This work aims to build a new semantic-based pipeline for recommending biomedical entities to scientific researchers. To this end, we developed a pipeline that creates an implicit feedback matrix based on Named Entity Recognition (NER) on a corpus of documents, using multidisciplinary ontologies for recognizing and linking the entities. Our hypothesis is that by using ontologies from different fields in the NER phase, we can improve the results for state-of-the-art collaborative-filtering recommender systems applied to the dataset created. The tests performed using the COVID-19 Open Research Dataset (CORD-19) dataset show that when using four ontologies, the results for precision@k, for example, reach the 80%, whereas when using only one ontology, the results for precision@k drops to 20%, for the same users. Furthermore, the use of multi-fields entities may help in the discovery of new items, even if the researchers do not have items from that field in their set of preferences.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 abbr"><abbr class="badge">Database</abbr></div>

  <!-- Entry bib key -->
  <div id="database" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">A hybrid approach toward biomedical relation extraction training corpora: combining distant supervision with crowdsourcing</div>
    <!-- Author -->
    <div class="author">
      

      Francisco M. Couto Diana F. Sousa</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Database</em>, 2020
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://academic.oup.com/database/article/doi/10.1093/database/baaa104/6013761" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://watermark.silverchair.com/baaa104.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA24wggNqBgkqhkiG9w0BBwagggNbMIIDVwIBADCCA1AGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMF1AQrU0tuFBieZpBAgEQgIIDIZPoQFldYm6MeKsdc9GUmsqvTG42X2u8-X-NL8TCx1_6NOtbO_-DREQNdpC4ohmVu_UTDD6XPegAM9rCJgikyrRAdsL-wWX20MPJDY5I0p6XIQVP8ChNPNpmt9I61QeWqXtUUHq_hMnywa0P5aHZv_i9psFPiUwBEBEe8fb9ArdihbZQCIqdP188jydLHgMjzNgY8E4JccVdsj2QEPv_IPkJnr6TaJb7B1GEL4a9LQmF4pyx_G6PvcYLqxa80pvdUdKcQWnStaQTjB1nz_K-XCuMCYUEJrWSbvf1v9mS4wDzM6vQJrq31x-dPCIm6HH00S4Wpjva3ACUk67tdjUQUN0cC-oY2oW5AY1I65WnVYHXtTRP44zi6-PbhJFDfjsz6CDNfeYTm_5NEq2FHyqG9jM15QmXVwEe7DWMd6uh8QC7ew9fttsLtz0olIUOCmzK6GEjOsJzH1vjgpGGhULup88ZjE5yli21wZNWxn2Rg5pWchsxI5bUgG5QGTNU%E2%80%93fU3utJ_4zh3cUOYEDEFNmmToXRf6bOryDdNkPZ8cyAVhorABCMD3mxtO6ob9M-aMK9W8exPD2M_HOfWjKndNkonfHYZhi6AeHeyib1I8PCdsotNPislr4V43HCqfYsXg7qCWEM8OJvGCqPSpjlWT2hmm0__LhjyvQFgADo17avFWJJ13clHRYQEiAQBRYSjCny6h4zA2uwlDoZY_mD_Gk9l-znkpPDsvVrtwNq3JeC4xCcNRPL5I8OghvMXTpBoq7hV8OO4lDEDUgD9bgqtZHR1EtaVrpZmuPl4st6DF04fmZlwZkb8jS3QFGKYfh9T3qN_rics3RUbFbhSHY6bGTLjBYLL1QEl4654eUTPba97hsK-5OAMjRHllJhj51Ki2u0LaWtSV989Ms_gDmkHKzhF5G7ZXZ4yyKnhms8g2p1bmyK4JKnnGkISqz7vxVk8Tlh3kz09gWXUFYtOZ-H4l7FxeI79MnW0Q8t6y2vwjjXAue0i60xOwDZgT7g37CpO6gp5uIT4tmWvxr8VZHmHg53RY3j7YamdeNQLsIzFkjbMHShdg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="https://github.com/lasigeBioTM/PGR-crowd" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Biomedical relation extraction (RE) datasets are vital in the construction of knowledge bases and to potentiate the discovery of new interactions. There are several ways to create biomedical RE datasets, some more reliable than others, such as resorting to domain expert annotations. However, the emerging use of crowdsourcing platforms, such as Amazon Mechanical Turk (MTurk), can potentially reduce the cost of RE dataset construction, even if the same level of quality cannot be guaranteed. There is a lack of power of the researcher to control who, how and in what context workers engage in crowdsourcing platforms. Hence, allying distant supervision with crowdsourcing can be a more reliable alternative. The crowdsourcing workers would be asked only to rectify or discard already existing annotations, which would make the process less dependent on their ability to interpret complex biomedical sentences. In this work, we use a previously created distantly supervised human phenotypeâ€“gene relations (PGR) dataset to perform crowdsourcing validation. We divided the original dataset into two annotation tasks: Task 1, 70% of the dataset annotated by one worker, and Task 2, 30% of the dataset annotated by seven workers. Also, for Task 2, we added an extra rater on-site and a domain expert to further assess the crowdsourcing validation quality. Here, we describe a detailed pipeline for RE crowdsourcing validation, creating a new release of the PGR dataset with partial domain expert revision, and assess the quality of the MTurk platform. We applied the new dataset to two state-of-the-art deep learning systems (BiOnt and BioBERT) and compared its performance with the original PGR dataset, as well as combinations between the two, achieving a 0.3494 increase in average F-measure. The code supporting our work and the new release of the PGR dataset is available at https://github.com/lasigeBioTM/PGR-crowd.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/"><div id="-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('-modal').style.display='none'">Ã—</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('');
      var modalImg = document.getElementById("-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="ecir" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Deep Learning System for Biomedical Relation Extraction Combining External Sources of Knowledge</div>
    <!-- Author -->
    <div class="author">
      

      Diana F. Sousa</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Advances in Information Retrieval: 43rd European Conference on IR Research</em>, 2021
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://link.springer.com/chapter/10.1007/978-3-030-72240-1_82" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Successful biomedical relation extraction can provide evidence to researchers about possible unknown associations between entities, advancing our current knowledge about those entities and their inherent processes. Multiple relation extraction approaches have been proposed to identify relations between concepts in literature, namely using neural networks algorithms. However, the incorporation of semantics is still scarce. This project proposes that using external semantic sources of knowledge along with the latest state-of-the-art language representations can improve the current performance of biomedical relation extraction both in English and non-English languages. The goal is to build a relation extraction system using state-of-the-art language representations, such as BERT and ELMo, with semantics retrieved from external sources of knowledge, such as domain-specific ontologies, graph attention mechanisms, and semantic similarity measures.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/"><div id="-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('-modal').style.display='none'">Ã—</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('');
      var modalImg = document.getElementById("-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="g&amp;I" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">COVID-19 recommender system based on an annotated multilingual corpus</div>
    <!-- Author -->
    <div class="author">
      

      </div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Genomics &amp; Informatics</em>, 2021
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://pubmed.ncbi.nlm.nih.gov/34638171/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://genominfo.org/upload/pdf/gi-21008.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">Slides</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Tracking the most recent advances in Coronavirus disease 2019 (COVID-19)-related research is essential, given the diseaseâ€™s novelty and its impact on society. However, with the publication pace speeding up, researchers and clinicians require automatic approaches to keep up with the incoming information regarding this disease. A solution to this problem requires the development of text mining pipelines; the efficiency of which strongly depends on the availability of curated corpora. However, there is a lack of COVID-19-related corpora, even more, if considering other languages besides English. This projectâ€™s main contribution was the annotation of a multilingual parallel corpus and the generation of a recommendation dataset (EN-PT and EN-ES) regarding relevant entities, their relations, and recommendation, providing this resource to the community to improve the text mining research on COVID-19-related literature. This work was developed during the 7th Biomedical Linked Annotation Hackathon (BLAH7).</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/"><div id="-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('-modal').style.display='none'">Ã—</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('');
      var modalImg = document.getElementById("-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="BioCreative" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">lasigeBioTM at BioCreative VII Track 1: Text mining drug and chemical-protein interactions using biomedical ontologies</div>
    <!-- Author -->
    <div class="author">
      

      Rodrigo Cassanheira Diana F. Sousa,Â andÂ Francisco M. Couto</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Proceedings of the BioCreative VII Challenge Evaluation Workshop</em>, 2021
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://www.semanticscholar.org/paper/lasigeBioTM-at-BioCreative-VII-Track-1%3A-Text-mining-Sousa-Cassanheira/07cd93f1bbb49c505254dc9368941867b6418839" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://biocreative.bioinformatics.udel.edu/media/store/files/2021/Track1_pos_18_BC7_submission_174.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">Slides</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Identifying biomedical relations is necessary to advance our understanding of biological processes and is particularly relevant for applications in precision medicine. This work describes the participation of the lasigeBioTM team in the BioCreative VII Track 1, whose primary goal is the extraction and classification of drug and chemical-protein interactions. Our team adapted an existing neural networks system, BiOnt, that incorporates external knowledge from biomedical ontologies. To perform Track 1, we used the Gene Ontology (GO) and the Chemical Entities of Biological Interest (ChEBI) ontology. We submitted different runs taking into account the use of features such as class weights and post-processing rules. However, due to time constraints, we could not make all the improvements that we planned initially, and our results were below the mean performance of the participating teams. Still, we took the first steps towards this adaption and we are now able to continue improving this system to reach state-of-the-art performance. Keywordsâ€”biomedical relation extraction; text mining; deep learning; external knowledge</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 abbr"><abbr class="badge">IEEE</abbr></div>

  <!-- Entry bib key -->
  <div id="IEEE" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Biomedical Relation Extraction with Knowledge Graph-based Recommendations</div>
    <!-- Author -->
    <div class="author">
      

      Diana F. Sousa,Â andÂ Francisco M. Couto</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>IEEE Journal of Biomedical and Health Informatics</em>, 2022
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://ieeexplore.ieee.org/document/9772412" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9772412" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="https://github.com/lasigeBioTM/K-BiOnt" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Biomedical Relation Extraction (RE) systems identify and classify relations between biomedical entities to enhance our knowledge of biological and medical processes. Most state-of-the-art systems use deep learning approaches, mainly to target relations between entities of the same type, such as proteins or pharmacological substances. However, these systems are mostly restricted to what they directly identify on the text and ignore specialized domain knowledge bases, such as ontologies, that formalize and integrate biomedical information typically structured as direct acyclic graphs. On the other hand, Knowledge Graph (KG)-based recommendation systems already showed the importance of integrating KGs to add additional features to items. Typical systems have users as people and items that can range from movies to books, which people saw or read and classified according to their satisfaction rate. This work proposes to integrate KGs into biomedical RE through a recommendation model to further improve their range of action. We developed a new RE system, named K-BiOnt, by integrating a baseline state-of-the-art deep biomedical RE system with an existing KG-based recommendation state-of-the-art system. Our results show that adding recommendations from KG-based recommendation improves the systemâ€™s ability to identify true relations that the baseline deep RE model could not extract from the text. The code supporting this system is available at https://github.com/lasigeBioTM/K-BiOnt.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="bio_inf_bookjpeg" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/bio_inf_book.jpeg"><div id="bio_inf_bookjpeg-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('bio_inf_bookjpeg-modal').style.display='none'">Ã—</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="bio_inf_bookjpeg-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("bio_inf_bookjpeg-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('bio_inf_bookjpeg');
      var modalImg = document.getElementById("bio_inf_bookjpeg-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="chapter" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">K-RET: knowledgeable biomedical relation extraction system</div>
    <!-- Author -->
    <div class="author">
      

      Francisco M Couto Diana F. Sousa</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Bioinformatics</em>, 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://academic.oup.com/bioinformatics/article/39/4/btad174/7108769" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://watermark.silverchair.com/btad174.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA3EwggNtBgkqhkiG9w0BBwagggNeMIIDWgIBADCCA1MGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMWVrMDrRtoE-c2Zi_AgEQgIIDJFNliRwNWtuUkCoY8Y0IW27NDbvyR-fRPwAodq9M9iD9vejvMw-7ts04qx6f6JXOMCRlFMIvTSl6Exm81JjB2lSLHTrMjHVeQowEB6ed_XqEJ6-_D3ab_mFT0gNWV9r7WzyyU0bt20w8BJwLoLLD3JVeqfwrKqTqP1nfMXOsPcMARTaUox8Ima8KvEQHK-b1Gpm6WAoBSj9MN9nFliru49n2CXKqaeTqiZxwyod_sBwTLWm31o_VNxSJqHpUXRwDG1izOoX6lXPb5tBmf5igrOXR8EobcgpnfQkZneHtKX2pSbWfm5g-bqaiyo9rs2qbHKh9CFpX61oZoGFDD9HzABkzD8zmzh8R8c-KubHTUtOZ4RLqU-Ukc7M2YRuAcziMxHBmPz9_kaDnhKWJTjQ7ssSrKDTNCPCnrwulJhnWbtkM4EhIOy6Cw_wd6H6tM5p7ug5g0YaumYhLZBC-lG8uZ6N_Fa7Ou5MmN7FFHb4wNPqPWSemlmeDv7IUnhDK4ytz6TqxUn7nXeLq1wy7O2SpREEjIVNMsYXCE8V4p1SC5ykFtvJ-6Tks4gJk_G1APLBAhd_545SXu5P0tdCThhrR7V_RzXI1XTK7-XYOMebmeZA0opkRYx4kIDT8epVGhYLB66Q-98HrquZLVMgn-LhmH7O2DqSC8qxAswp-YGjcvOnVWzPJWeOOE9FG5CN1CeNSy87hRZRBSid-MzUnOhdW9Ozvaa4pAZxyu9ZlZY0p3EQaOoDHZyKr2mENWvY-qKtyS5wYgBdVwb35XIrJbov_M9VBhbFrKxxFE9PNB-NgfnJTi5Rdm8aYHAOs6VdmRVgcj9zOw3UfJ4MD9V7La6QI9iTsK-E-Eu-8oUfe-ABnobQsLUCEw_SzoWWXgeVSGPlZxCeJmTLT_FqglJSd07aj2KDnwEfa9XavGVD7r33ZnXCMXg1Pyg8pqTt8hWP9asV3F2gfUK4i3UOPFhP7P8fIjfGQ2SsFYDrxSM-JaE194dyLX6HtwVs-SOgbdLQsXIg7cO6K4nsCBdyVSzxsabzss6UiI_PZUSPnrIn1lFBCIf3gKCttZg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Relation extraction (RE) is a crucial process to deal with the amount of text published daily, e.g. to find missing associations in a database. RE is a text mining task for which the state-of-the-art approaches use bidirectional encoders, namely, BERT. However, state-of-the-art performance may be limited by the lack of efficient external knowledge injection approaches, with a larger impact in the biomedical area given the widespread usage and high quality of biomedical ontologies. This knowledge can propel these systems forward by aiding them in predicting more explainable biomedical associations. With this in mind, we developed K-RET, a novel, knowledgeable biomedical RE system that, for the first time, injects knowledge by handling different types of associations, multiple sources and where to apply it, and multi-token entities. We tested K-RET on three independent and open-access corpora (DDI, BC5CDR, and PGR) using four biomedical ontologies handling different entities. K-RET improved state-of-the-art results by 2.68% on average, with the DDI Corpus yielding the most significant boost in performance, from 79.30% to 87.19% in F-measure, representing a P-value of 2.91E-12â .</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 abbr"><abbr class="badge">SemEval</abbr></div>

  <!-- Entry bib key -->
  <div id="SemEvam" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">lasigeBioTM at SemEval-2023 Task 7: Improving Natural Language Inference Baseline Systems with Domain Ontologies</div>
    <!-- Author -->
    <div class="author">
      

      </div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Proceedings of the 17th International Workshop on Semantic Evaluation</em>, 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://aclanthology.org/2023.semeval-1.2/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://aclanthology.org/2023.semeval-1.2.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Clinical Trials Reports (CTRs) contain highly valuable health information from which Natural Language Inference (NLI) techniques determine if a given hypothesis can be inferred from a given premise. CTRs are abundant with domain terminology with particular terms that are difficult to understand without prior knowledge. Thus, we proposed to use domain ontologies as a source of external knowledge that could help with the inference process in the SemEval-2023 Task 7: Multi-evidence Natural Language Inference for Clinical Trial Data (NLI4CT). This document describes our participation in subtask 1: Textual Entailment, where Ontologies, NLP techniques, such as tokenization and namedentity recognition, and rule-based approaches are all combined in our approach. We were able to show that inputting annotations from domain ontologies improved the baseline systems.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 abbr"><abbr class="badge">BioRED</abbr></div>

  <!-- Entry bib key -->
  <div id="BioRED" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">BioRED track lasigeBioTM submission: Relation Extraction using Domain Ontologies with BioRED</div>
    <!-- Author -->
    <div class="author">
      

      </div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Proceedings of the BioCreative VIII Challenge and Workshop: Curation and Evaluation in the era of Generative Models</em>, 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://zenodo.org/records/10118137" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://zenodo.org/records/10118137/preview/BioRED%20track%20lasigeBioTM%20submission%20Relation%20Extraction%20using%20Domain%20Ontologies%20with%20BioRED.pdf?include_deleted=0" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Biomedical relation extraction is a crucial task for extracting valuable knowledge from unstructured scientific literature. This paper discusses our teamâ€™s, lasigeBioTM, involvement in the BioCreative VIII Track 1: BioRED in both sub-tasks. Our primary focus was on the relation extraction (RE) task, taking advantage of the K-RET system in combination with Gene Ontology, Chemical Entities of Biological Interest, Human Phenotype Ontology, Human Disease Ontology and NCBITaxon Ontology. The objective was to evaluate whether the use of external knowledge could enhance the performance of the relation extraction task, both for entity relationships and for detecting novel information. Our results in both tasks were below the average and we were not able to discern the impact of the introduced external knowledge. However, it was observed that for our model, a cleaner dataset is needed for improved performance and the necessity for a larger number of example instances, as our model struggled to identify low-represented labels.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="magnetgif" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/magnet.gif"><div id="magnetgif-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('magnetgif-modal').style.display='none'">Ã—</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="magnetgif-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("magnetgif-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('magnetgif');
      var modalImg = document.getElementById("magnetgif-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="Magnet" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">MAGNET: Multi-Label Text Classification using Attention-based Graph Neural Network</div>
    <!-- Author -->
    <div class="author">
      

      <em>Ankit Pal</em>,Â <a href="https://scholar.google.com/citations?user=_KPtoVcAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Muru Selvakumar</a>,Â andÂ <a href="https://scholar.google.com/citations?user=znWv6tUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Malaikannan Sankarasubbu</a>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Proceedings of the 12th International Conference on Agents and Artificial Intelligence</em>, 2020
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://doi.org/10.5220%2F0008940304940505" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://arxiv.org/pdf/2003.11644.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="https://github.com/monk1337/ResearchSlides/blob/main/Magnet_paper/Presentation_slides.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>In Multi-Label Text Classification (MLTC), one sample can belong to more than one class. It is observed that most MLTC tasks, there are dependencies or correlations among labels. Existing methods tend to ignore the relationship among labels. In this paper, a graph attention network-based model is proposed to capture the attentive dependency structure among the labels. The graph attention network uses a feature matrix and a correlation matrix to capture and explore the crucial dependencies between the labels and generate classifiers for the task. The generated classifiers are applied to sentence feature vectors obtained from the text feature extraction network (BiLSTM) to enable end-to-end training. Attention allows the system to assign different weights to neighbor nodes per label, thus allowing it to learn the dependencies among labels implicitly. The results of the proposed model are validated on five real-world MLTC datasets. The proposed model achieves similar or better performance compared to the previous state-of-the-art models.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="coughpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/cough.png"><div id="coughpng-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('coughpng-modal').style.display='none'">Ã—</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="coughpng-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("coughpng-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('coughpng');
      var modalImg = document.getElementById("coughpng-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="Cough" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Pay Attention to the cough: Early Diagnosis of COVID-19 using Interpretable Symptoms Embeddings with Cough Sound Signal Processing</div>
    <!-- Author -->
    <div class="author">
      

      <em>Ankit Pal</em>,Â andÂ <a href="https://scholar.google.com/citations?user=znWv6tUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Malaikannan Sankarasubbu</a>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Proceedings of the 36th Annual ACM Symposium on Applied Computing</em>, 2021
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://dl.acm.org/doi/10.1145/3412841.3441943" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://dl.acm.org/doi/pdf/10.1145/3412841.3441943" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="https://github.com/coughresearch/Cough-signal-processing" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="https://github.com/monk1337/ResearchSlides/blob/main/Cough_paper/presentation_slides.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a>
      <a href="https://coughresearch.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>COVID-19 (coronavirus disease 2019) pandemic caused by SARS-CoV-2 has led to a treacherous and devastating catastrophe for humanity. At the time of writing, no specific antivirus drugs or vaccines are recommended to control infection transmission and spread. The current diagnosis of COVID-19 is done by Reverse-Transcription Polymer Chain Reaction (RT-PCR) testing. However, this method is expensive, time-consuming, and not easily available in straitened regions. An interpretable and COVID-19 diagnosis AI framework is devised and developed based on the cough sounds features and symptoms metadata to overcome these limitations. The proposed frameworkâ€™s performance was evaluated using a medical dataset containing Symptoms and Demographic data of 30000 audio segments, 328 cough sounds from 150 patients with four cough classes ( COVID-19, Asthma, Bronchitis, and Healthy). Experimentsâ€™ results show that the model captures the better and robust feature embedding to distinguish between COVID-19 patient coughs and several types of non-COVID-19 coughs with higher specificity and accuracy of 95.04 Â± 0.18% and 96.83Â± 0.18% respectively, all the while maintaining interpretability.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div>

  <!-- Entry bib key -->
  <div id="Clift" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">CLIFT: Analysing Natural Distribution Shift on Question Answering Models in Clinical Domain</div>
    <!-- Author -->
    <div class="author">
      

      <em>Ankit Pal</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>NeurIPS-2022: Robustness in Sequence Modeling</em>
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://nips.cc/virtual/2022/58229" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://openreview.net/pdf?id=9PQFROOfqm" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="https://nips.cc/media/PosterPDFs/NeurIPS%202022/58229.png?t=1668359616.0178533" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Poster</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>This paper introduces a new testbed CLIFT (Clinical Shift) for the clinical domain Question Answering task. The testbed includes 7.5k high-quality questionanswering samples to provide a diverse and reliable benchmark. We performed a comprehensive experimental study and evaluated several QA deep-learning models under the proposed testbed. Despite impressive results on the original test set, the performance degrades when applied to new test sets, which shows the distribution shift. Our findings emphasize the need for and the potential for increasing the robustness of clinical domain models under distributional shifts. The testbed offers one way to track progress in that direction. It also highlights the necessity of adopting evaluation metrics that consider robustness to natural distribution shifts. We plan to expand the corpus by adding more samples and model results. The full paper and the updated benchmark are available at openlifescience-ai.github.io/clift</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="medmcqapng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/medmcqa.png"><div id="medmcqapng-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('medmcqapng-modal').style.display='none'">Ã—</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="medmcqapng-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("medmcqapng-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('medmcqapng');
      var modalImg = document.getElementById("medmcqapng-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="MedMCQA" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering</div>
    <!-- Author -->
    <div class="author">
      

      <em>Ankit Pal</em>,Â <a href="https://scholar.google.com/citations?user=eWwEqToAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Logesh Kumar Umapathi</a>,Â andÂ <a href="https://scholar.google.com/citations?user=znWv6tUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Malaikannan Sankarasubbu</a>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Proceedings of Machine Learning Research(PMLR)</em>, 2022
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://proceedings.mlr.press/v174/pal22a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://proceedings.mlr.press/v174/pal22a/pal22a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="https://github.com/medmcqa/medmcqa" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="https://github.com/monk1337/ResearchSlides/blob/main/MedMCQA_paper/presentation_slides.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>This paper introduces MedMCQA, a new large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions. More than 194k high-quality AIIMS &amp; NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity. Each sample contains a question, correct answer(s), and other options which requires a deeper language understanding as it tests the 10+ reasoning abilities of a model across a wide range of medical subjects &amp; topics. A detailed explanation of the solution, along with the above information, is provided in this study.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="fedlearnpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/fedlearn.png"><div id="fedlearnpng-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('fedlearnpng-modal').style.display='none'">Ã—</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="fedlearnpng-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("fedlearnpng-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('fedlearnpng');
      var modalImg = document.getElementById("fedlearnpng-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="FedLearn" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Federated learning for healthcare domain - pipeline, applications and challenges</div>
    <!-- Author -->
    <div class="author">
      

      <a href="https://madhura12gj.github.io/MadhuraJoshi/" rel="external nofollow noopener" target="_blank">Madhura Joshi*</a>,Â <em>Ankit Pal*</em>,Â andÂ <a href="https://scholar.google.com/citations?user=znWv6tUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Malaikannan Sankarasubbu</a>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>ACM Transactions on Computing for Healthcare</em>, 2022
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://dl.acm.org/doi/10.1145/3533708" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://dl.acm.org/doi/pdf/10.1145/3533708" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="https://github.com/monk1337/ResearchSlides/blob/main/Federated_learning_paper/presentation_slides.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slides</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>Federated learning is the process of developing machine learning models over datasets distributed across data centers such as hospitals, clinical research labs, and mobile devices while preventing data leakage. This survey examines previous research and studies on federated learning in the healthcare sector across a range of use cases and applications. Our survey shows what challenges, methods, and applications a practitioner should be aware of in the topic of federated learning. This paper aims to lay out existing research and list the possibilities of federated learning for healthcare industries.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div>

  <!-- Entry bib key -->
  <div id="DeepParliament" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">DeepParliament: A Legal domain Benchmark &amp; Dataset for Parliament Bills Prediction</div>
    <!-- Author -->
    <div class="author">
      

      <em>Ankit Pal</em>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Empirical Methods in Natural Language Processing(UM-IoS)</em>, 2022
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://aclanthology.org/2022.umios-1.8/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://aclanthology.org/2022.umios-1.8.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="https://github.com/monk1337/DeepParliament/tree/main" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>This paper introduces DeepParliament, a legal domain Benchmark Dataset that gathers bill documents and metadata and performs various bill status classification tasks. The proposed dataset text covers a broad range of bills from 1986 to the present and contains richer information on parliament bill content. Data collection, detailed statistics and analyses are provided in the paper. Moreover, we experimented with different types of models ranging from RNN to pretrained and reported the results. We are proposing two new benchmarks: Binary and Multi-Class Bill Status classification. Models developed for bill documents and relevant supportive tasks may assist Members of Parliament (MPs), presidents, and other legal practitioners. It will help review or prioritise bills, thus speeding up the billing process, improving the quality of decisions and reducing the time consumption in both houses. Considering that the foundation of the countryâ€s democracy is Parliament and state legislatures, we anticipate that our research will be an essential addition to the Legal NLP community. This work will be the first to present a Parliament bill prediction task. In order to improve the accessibility of legal AI resources and promote reproducibility, we have made our code and dataset publicly accessible at github.com/monk1337/DeepParliament.</p>
    </div>
  </div>
</div>
</li>
<li>
<!-- _layouts/bib.html -->
<div class="row">
  <div class="col-sm-2 preview">
<img id="medhaltpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/medhalt.png"><div id="medhaltpng-modal" class="modal2">
      <span class="closeimg" onclick="document.getElementById('medhaltpng-modal').style.display='none'">Ã—</span>
      <div class="myCaption"></div>
      <img class="modal-content2" id="medhaltpng-modal-img">
    </div>
    <script>
      // Get the modal
      var modal = document.getElementById("medhaltpng-modal");
      // Get the image and insert it inside the modal - use its "alt" text as a caption
      var img = document.getElementById('medhaltpng');
      var modalImg = document.getElementById("medhaltpng-modal-img");
      var captionText = modal.getElementsByTagName('div')[0];
      img.onclick = function () {
        modal.style.display = "block";
        modalImg.src = this.src;
        captionText.innerHTML = "";
      }
      // Get the <span> element that closes the modal
      var span = modal.getElementsByTagName('span')[0];
      // When the user clicks on <span> (x), close the modal
      span.onclick = function () {
        modal.style.display = "none";
      }
      modal.onclick = function () {
        modal.style.display = "none";
      }
    </script>
</div>

  <!-- Entry bib key -->
  <div id="MedHALT" class="col-sm-8 col-md-9">
    <!-- Title -->
    <div class="title">Med-HALT: Medical Domain Hallucination Test for Large Language Models</div>
    <!-- Author -->
    <div class="author">
      

      <em>Ankit Pal</em>,Â <a href="https://scholar.google.com/citations?user=eWwEqToAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Logesh Kumar Umapathi</a>,Â andÂ <a href="https://scholar.google.com/citations?user=znWv6tUAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Malaikannan Sankarasubbu</a>
</div>

    <!-- Journal/Book title and date -->
    
    
    <div class="periodical">
      <em>Empirical Methods in Natural Language Processing(Conll)</em>, 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      <a href="https://arxiv.org/abs/2307.15343" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">URL</a>
      <a href="https://arxiv.org/pdf/2307.15343.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
      <a href="https://github.com/medhalt/medhalt" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
      <a href="/assets/pdf/" class="btn btn-sm z-depth-0" role="button">Slides</a>
    </div>
    
    <div class="badges">
    </div>

    <!-- Hidden abstract block -->
    <div class="abstract hidden">
      <p>This research paper focuses on the challenges posed by hallucinations in large language models (LLMs), particularly in the context of the medical domain. Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain Hallucination Test), designed specifically to evaluate and reduce hallucinations. Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMsâ€™s problem-solving and information retrieval abilities. Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance. The paper provides detailed insights into the dataset, promoting transparency and reproducibility. Through this work, we aim to contribute to the development of safer and more reliable language models in healthcare. Our benchmark can be found at medhalt.github.io</p>
    </div>
  </div>
</div>
</li>
</ol>
          </div>


          <!-- Social -->
            <div class="social">
              <div class="contact-icons">
                <a href="mailto:%44%69%61%6E%61.%66%72%61%6E%63%69%73%63%6F-%64%65-%73%6F%75%73%61@%65%63.%65%75%72%6F%70%61.%65%75" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://orcid.org/0000-0003-0597-9273" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a>
            <a href="https://scholar.google.com/citations?user=6CTPQnsAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://www.researchgate.net/profile/Diana-Sousa-6/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a>
            <a href="https://github.com/dpavot" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/diana-sousa" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a>
            <a href="https://twitter.com/dianafdsousa" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a>
            <a href="https://dblp.org/pid/351/3594.html" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a>
            

              </div>

              <div class="contact-note">
                
              </div>

            </div>
        </article>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        Â© Copyright 2024 Diana F. Sousa, PhD. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme.
Last updated: January 30, 2024.
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-09S9K95MD5"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-09S9K95MD5');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
